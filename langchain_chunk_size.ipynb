{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kariske/langchain/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=os.environ.get('OPENAI_API_KEY'),\n",
    "    openai_api_base=os.environ.get('CHATGPT_API_ENDPOINT')   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import ReadTheDocsLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = ReadTheDocsLoader(\"htmldocs\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain.indexes.vectorstore.VectorstoreIndexCreator¶\n",
      "class langchain.indexes.vectorstore.VectorstoreIndexCreator[source]¶\n",
      "Bases: BaseModel\n",
      "Logic for creating indexes.\n",
      "Create a new model by parsing and validating input data from keyword arguments.\n",
      "Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "param embedding: Embeddings [Optional]¶\n",
      "param text_splitter: TextSplitter [Optional]¶\n",
      "param vectorstore_cls: Type[VectorStore] = <class 'langchain_community.vectorstores.\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt-3.5-turbo 4096 tokens\n",
    "# If 4096 - (Input(Instruction + query + context) + output)\n",
    "#     If Chunk nums = 5:\n",
    "#         Chunk Size = 2000 / 5 = 400\n",
    "\n",
    "# So Chunk Size <= 400\n",
    "\n",
    "# Too small not meaningful\n",
    "# Too big not efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'cl100k_base'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_count(text):\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        disallowed_special=()\n",
    "    )\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1538, 1605]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [token_count(doc.page_content) for doc in docs]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=20,\n",
    "    length_function=token_count,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = text_splitter.split_text(docs[0].page_content)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(383, 373, 345, 376, 105)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_count(chunks[0]), token_count(chunks[1]), token_count(chunks[2]), token_count(chunks[3]), token_count(chunks[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"langchain.indexes.vectorstore.VectorstoreIndexCreator¶\\nclass langchain.indexes.vectorstore.VectorstoreIndexCreator[source]¶\\nBases: BaseModel\\nLogic for creating indexes.\\nCreate a new model by parsing and validating input data from keyword arguments.\\nRaises ValidationError if the input data cannot be parsed to form a valid model.\\nparam embedding: Embeddings [Optional]¶\\nparam text_splitter: TextSplitter [Optional]¶\\nparam vectorstore_cls: Type[VectorStore] = <class 'langchain_community.vectorstores.inmemory.InMemoryVectorStore'>¶\\nparam vectorstore_kwargs: dict [Optional]¶\\nasync afrom_documents(documents: List[Document]) → VectorStoreIndexWrapper[source]¶\\nCreate a vectorstore index from documents.\\nParameters\\ndocuments (List[Document]) – \\nReturn type\\nVectorStoreIndexWrapper\\nasync afrom_loaders(loaders: List[BaseLoader]) → VectorStoreIndexWrapper[source]¶\\nCreate a vectorstore index from loaders.\\nParameters\\nloaders (List[BaseLoader]) – \\nReturn type\\nVectorStoreIndexWrapper\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model¶\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‘allow’ was set since it adds all passed values\\nParameters\\n_fields_set (Optional[SetStr]) – \\nvalues (Any) – \\nReturn type\\nModel\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model¶\",\n",
       " 'Duplicate a model, optionally choose which fields to include, exclude and change.\\nParameters\\ninclude (Optional[Union[AbstractSetIntStr, MappingIntStrAny]]) – fields to include in new model\\nexclude (Optional[Union[AbstractSetIntStr, MappingIntStrAny]]) – fields to exclude from new model, as with values this takes precedence over include\\nupdate (Optional[DictStrAny]) – values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep (bool) – set to True to make a deep copy of the model\\nself (Model) – \\nReturns\\nnew model instance\\nReturn type\\nModel\\ndict(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False) → DictStrAny¶\\nGenerate a dictionary representation of the model, optionally specifying which fields to include or exclude.\\nParameters\\ninclude (Optional[Union[AbstractSetIntStr, MappingIntStrAny]]) – \\nexclude (Optional[Union[AbstractSetIntStr, MappingIntStrAny]]) – \\nby_alias (bool) – \\nskip_defaults (Optional[bool]) – \\nexclude_unset (bool) – \\nexclude_defaults (bool) – \\nexclude_none (bool) – \\nReturn type\\nDictStrAny\\nfrom_documents(documents: List[Document]) → VectorStoreIndexWrapper[source]¶\\nCreate a vectorstore index from documents.\\nParameters\\ndocuments (List[Document]) – \\nReturn type\\nVectorStoreIndexWrapper',\n",
       " 'Parameters\\ndocuments (List[Document]) – \\nReturn type\\nVectorStoreIndexWrapper\\nfrom_loaders(loaders: List[BaseLoader]) → VectorStoreIndexWrapper[source]¶\\nCreate a vectorstore index from loaders.\\nParameters\\nloaders (List[BaseLoader]) – \\nReturn type\\nVectorStoreIndexWrapper\\nclassmethod from_orm(obj: Any) → Model¶\\nParameters\\nobj (Any) – \\nReturn type\\nModel\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode¶\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\nParameters\\ninclude (Optional[Union[AbstractSetIntStr, MappingIntStrAny]]) – \\nexclude (Optional[Union[AbstractSetIntStr, MappingIntStrAny]]) – \\nby_alias (bool) – \\nskip_defaults (Optional[bool]) – \\nexclude_unset (bool) – \\nexclude_defaults (bool) – \\nexclude_none (bool) – \\nencoder (Optional[Callable[[Any], Any]]) – \\nmodels_as_dict (bool) – \\ndumps_kwargs (Any) – \\nReturn type\\nunicode',\n",
       " \"dumps_kwargs (Any) – \\nReturn type\\nunicode\\nclassmethod parse_file(path: Union[str, Path], *, content_type: unicode = None, encoding: unicode = 'utf8', proto: Protocol = None, allow_pickle: bool = False) → Model¶\\nParameters\\npath (Union[str, Path]) – \\ncontent_type (unicode) – \\nencoding (unicode) – \\nproto (Protocol) – \\nallow_pickle (bool) – \\nReturn type\\nModel\\nclassmethod parse_obj(obj: Any) → Model¶\\nParameters\\nobj (Any) – \\nReturn type\\nModel\\nclassmethod parse_raw(b: Union[str, bytes], *, content_type: unicode = None, encoding: unicode = 'utf8', proto: Protocol = None, allow_pickle: bool = False) → Model¶\\nParameters\\nb (Union[str, bytes]) – \\ncontent_type (unicode) – \\nencoding (unicode) – \\nproto (Protocol) – \\nallow_pickle (bool) – \\nReturn type\\nModel\\nclassmethod schema(by_alias: bool = True, ref_template: unicode = '#/definitions/{model}') → DictStrAny¶\\nParameters\\nby_alias (bool) – \\nref_template (unicode) – \\nReturn type\\nDictStrAny\\nclassmethod schema_json(*, by_alias: bool = True, ref_template: unicode = '#/definitions/{model}', **dumps_kwargs: Any) → unicode¶\\nParameters\\nby_alias (bool) – \\nref_template (unicode) – \\ndumps_kwargs (Any) – \\nReturn type\\nunicode\\nclassmethod update_forward_refs(**localns: Any) → None¶\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\nParameters\\nlocalns (Any) – \\nReturn type\\nNone\\nclassmethod validate(value: Any) → Model¶\\nParameters\",\n",
       " 'Return type\\nNone\\nclassmethod validate(value: Any) → Model¶\\nParameters\\nvalue (Any) – \\nReturn type\\nModel\\nExamples using VectorstoreIndexCreator¶\\nCreate a vectorstore retriever from the loader\\nRetrievers\\nSet env var OPENAI_API_KEY or load from a .env file:\\napify.md\\napify_dataset.md\\nhugging_face_dataset.md\\nimage_captions.md\\nsee https://python.langchain.com/en/latest/modules/data_connection/getting_started.html for more details']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedddings = OpenAIEmbeddings(\n",
    "    base_url=os.environ.get('CHATGPT_API_ENDPOINT'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences1 = \"Hello, my name is John.\"\n",
    "sentences2 = \"I am a software engineer.\"\n",
    "sentences3 = \"I love to code.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddding1 = embedddings.embed_query(sentences1)\n",
    "embeddding2 = embedddings.embed_query(sentences2)\n",
    "embeddding3 = embedddings.embed_query(sentences3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8136125713435753)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(embeddding1, embeddding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7818426246123964)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(embeddding1, embeddding3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "chromadb.api.client.SharedSystemClient.clear_system_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = \"./db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $persist_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_chunks = text_splitter.create_documents(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorDB = Chroma.from_documents(\n",
    "    documents=doc_chunks,\n",
    "    embedding=embedddings,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "# vectorDB = Chroma.from_texts(\n",
    "#     texts=chunks,\n",
    "#     embedding=embedddings,\n",
    "#     persist_directory=persist_directory\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(vectorDB._collection.count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
